# [modules/stt/azure_stt.py]
# Azure Speech to Text AI
import os
import azure.cognitiveservices.speech as speechsdk
import asyncio  #async ì„ ì–¸ì´ ì•ˆëœ ë™ê¸° í•¨ìˆ˜ì—ì„œ ë¹„ë™ê¸° í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•  ë•Œ í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬

class AzureSTT:
    # [1] ì´ˆê¸°í™”
    def __init__(self):
        # stt ë³€ìˆ˜ ì´ˆê¸°í™”
        self.azure_key = os.environ['AZURE_STT_KEY']
        self.azure_region = os.environ['AZURE_REGION']
        self.speech_recognizer = None
        self.audio_stream = None
        self.is_listening = False   # ìŒì„± ì¸ì‹ ì¤‘ë³µ ë°©ì§€
        self.result_queue = None    # stt ê²°ê³¼ ì €ì¥ í•¨ìˆ˜ (ë™ê¸° ì €ì¥ -> ë¹„ë™ê¸° ì¶”ì¶œ)

    # [2] ìŒì„± ì¸ì‹ ì„¤ì •
    def setup_streaming_recognition(self, input_language: str) : 
        """
        ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ìŒì„± ì¸ì‹ ì„¤ì •
        input_language : ì…ë ¥ ì–¸ì–´ ì½”ë“œ
        """

        # stt ê²°ê³¼ ì €ì¥ queue ìƒì„±
        if self.result_queue is None:
            self.result_queue = asyncio.Queue()

        #speech ì„¤ì •
        speech_config = speechsdk.SpeechConfig(
            subscription=self.azure_key,
            region=self.azure_region
        )
        #ì¸ì‹ ì–¸ì–´ ì„¤ì •
        speech_config.speech_recognition_language = input_language

        #ì˜¤ë””ì˜¤ ì„¤ì •
        audio_format = speechsdk.audio.AudioStreamFormat(
            samples_per_second=16000, 
            bits_per_sample=16, 
            channels=1
        )
        self.audio_stream = speechsdk.audio.PushAudioInputStream(audio_format)
        audio_config = speechsdk.audio.AudioConfig(stream=self.audio_stream)

        # ìŒì„± ì¸ì‹ê¸° ìƒì„±
        self.speech_recognizer = speechsdk.SpeechRecognizer(
            speech_config=speech_config,
            audio_config=audio_config
        )

        # ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬ ì„¤ì • 
        def recognized_handler(evt):
            if evt.result.reason == speechsdk.ResultReason.RecognizedSpeech:    #stt ê²°ê³¼ ë°›ì•„ì˜¤ê¸° -> ì²˜ë¦¬ ìì²´ë¥¼ ë™ê¸° ë°©ì‹ìœ¼ë¡œ ì§„í–‰ (ë¹„ë™ê¸° í•¨ìˆ˜ ì‚¬ìš©x)
                text = evt.result.text.strip()
                if text:
                    print(f"ğŸ—£ï¸ ì›ë³¸: {text}\n")
                    asyncio.create_task(self.result_queue.put(text))    #ë™ê¸° ë°©ì‹ìœ¼ë¡œ ë°˜í™˜ëœ stt ê²°ê³¼ë¥¼ queueì— ìˆœì„œëŒ€ë¡œ ì €ì¥

        def session_started_handler(evt):
            print("ğŸ¯ ìŒì„± ì¸ì‹ ì„¸ì…˜ì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.")
            
        def session_stopped_handler(evt):
            print("ğŸ›‘ ìŒì„± ì¸ì‹ ì„¸ì…˜ì´ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
            self.is_listening = False
            
        def canceled_handler(evt):
            print(f"âŒ ìŒì„± ì¸ì‹ì´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤: {evt.result.cancellation_details.reason}")
            if evt.result.cancellation_details.reason == speechsdk.CancellationReason.Error:
                print(f"ì˜¤ë¥˜ ì„¸ë¶€ì‚¬í•­: {evt.result.cancellation_details.error_details}")
            self.is_listening = False
        
        # ì´ë²¤íŠ¸ ì—°ê²°
        self.speech_recognizer.recognized.connect(recognized_handler)                   # stt ê²°ê³¼ê°€ ë‚˜ì™”ì„ ë•Œ,
        self.speech_recognizer.session_started.connect(session_started_handler)         # ì„¸ì…˜ì´ ì‹œì‘ë˜ì—ˆì„ ë•Œ, 
        self.speech_recognizer.session_stopped.connect(session_stopped_handler)         # ì„¸ì…˜ì´ ì¢…ë£Œë˜ì—ˆì„ ë•Œ,
        self.speech_recognizer.canceled.connect(canceled_handler)                       # ì¸ì‹ì´ ì·¨ì†Œë˜ì—ˆì„ ë–„,
    
    # [3] ìŒì„± ì¸ì‹ 
    def start_recognition(self):
        """ì—°ì† ìŒì„± ì¸ì‹ ì‹œì‘"""
        
        # ì¤‘ë³µ ì¸ì‹ ë°©ì§€
        if self.is_listening:
            print("âš ï¸ ì´ë¯¸ ìŒì„± ì¸ì‹ì´ ì§„í–‰ ì¤‘ì…ë‹ˆë‹¤.")
            return
        
        # ì—°ì† ì¸ì‹ ì‹œì‘
        self.speech_recognizer.start_continuous_recognition()   ## Azure STT audio_stream ëª¨ë‹ˆí„°ë§ ì‹œì‘ -> audio_data ì¶”ê°€ë˜ëŠ” ê²ƒ ì¸ì‹
        self.is_listening = True

    #[3-1] ì‹¤ì‹œê°„ ìŒì„± ë°›ì•„ì˜¤ê¸° : audio_streamì— ë°ì´í„° ì¶”ê°€ â†’ Azureê°€ ìë™ ê°ì§€ â†’ STT ì²˜ë¦¬ â†’ ì½œë°± í˜¸ì¶œ
    def write_audio_chunk(self, audio_data: bytes):
        """
        ì˜¤ë””ì˜¤ ì²­í¬ë¥¼ ìŠ¤íŠ¸ë¦¼ì— ì¶”ê°€
        
        Args:
            audio_data: ì˜¤ë””ì˜¤ ë°”ì´íŠ¸ ë°ì´í„°
        """
        if self.audio_stream and self.is_listening:
            self.audio_stream.write(audio_data)
    
    # [3-2] ìŒì„± ì²˜ë¦¬ ê²°ê³¼ queue ë¹„ë™ê¸°ë¡œ ë°˜í™˜
    async def get_recognition_result(self):
        """STT ê²°ê³¼ë¥¼ ë¹„ë™ê¸°ë¡œ ë°˜í™˜"""
        return await self.result_queue.get()

    # [4] ì‹¤í–‰ ì¤‘ì§€
    def stop_recognition(self):
        """ì—°ì† ìŒì„± ì¸ì‹ ì¤‘ì§€"""
        if self.speech_recognizer and self.is_listening:
            print("\nğŸ›‘ ìŒì„± ì¸ì‹ì„ ì¤‘ì§€í•©ë‹ˆë‹¤...")
            self.speech_recognizer.stop_recognition()
            self.is_listening = False
        else:
            print("âš ï¸ ì§„í–‰ ì¤‘ì¸ ìŒì„± ì¸ì‹ì´ ì—†ìŠµë‹ˆë‹¤.")

        if self.audio_stream:
            self.audio_stream.close()
            self.audio_stream = None

        print("âœ… ìŒì„± ì¸ì‹ ì¤‘ì§€ ì™„ë£Œ")

    # [5] ìŒì„± ì¸ì‹ ì–¸ì–´ ë³€ê²½
    def change_setup_recognition(self, input_language) : 
        self.stop_recognition()                                     # ê¸°ì¡´ ì¸ì‹ ì¤‘ì§€
        self.setup_streaming_recognition(input_language)  # ìŒì„± ì¸ì‹ ì–¸ì–´ ë³€ê²½
        self.start_recognition()                                    # ë‹¤ì‹œ ì‹œì‘

    def is_active(self) -> bool:
        """í˜„ì¬ ì¸ì‹ì´ í™œì„±í™”ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸"""
        return self.is_listening