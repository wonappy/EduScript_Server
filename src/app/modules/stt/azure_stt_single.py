# [modules/stt/azure_stt_single.py]
# Azure Speech to Text AI
import os
import azure.cognitiveservices.speech as speechsdk
import asyncio  #async ì„ ì–¸ì´ ì•ˆëœ ë™ê¸° í•¨ìˆ˜ì—ì„œ ë¹„ë™ê¸° í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•  ë•Œ í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬

class AzureSTTSingle:
    # [1] ì´ˆê¸°í™”
    def __init__(self):
        # stt ë³€ìˆ˜ ì´ˆê¸°í™”
        self.azure_key = os.environ['AZURE_STT_KEY']
        self.azure_region = os.environ['AZURE_REGION']
        self.speech_recognizer = None
        self.audio_stream = None
        self.is_listening = False   # ìŒì„± ì¸ì‹ ì¤‘ë³µ ë°©ì§€
        self.result_queue = None    # stt ê²°ê³¼ ì €ì¥ í•¨ìˆ˜ (ë™ê¸° ì €ì¥ -> ë¹„ë™ê¸° ì¶”ì¶œ)

    # [2] ìŒì„± ì¸ì‹ ì„¤ì •
    def setup_streaming_recognition(self, input_language: str) : 
        """
        ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ìŒì„± ì¸ì‹ ì„¤ì •
        input_language : ì…ë ¥ ì–¸ì–´ ì½”ë“œ
        """

        # stt ê²°ê³¼ ì €ì¥ queue ìƒì„±
        if self.result_queue is None:
            self.result_queue = asyncio.Queue()

        #speech ì„¤ì •
        speech_config = speechsdk.SpeechConfig(
            subscription=self.azure_key,
            region=self.azure_region
        )
        #ì¸ì‹ ì–¸ì–´ ì„¤ì •
        speech_config.speech_recognition_language = input_language

        speech_config.set_property(speechsdk.PropertyId.SpeechServiceConnection_InitialSilenceTimeoutMs, "3000")  # ë…¹ìŒ ì‹œì‘ í›„ ì²« ìŒì„±ì„ ê¸°ë‹¤ë¦¬ëŠ” ì‹œê°„
        speech_config.set_property(speechsdk.PropertyId.Speech_SegmentationSilenceTimeoutMs, "700")               # (ì„¸ê·¸ë¨¼íŠ¸)ë¬¸ì¥ êµ¬ë¶„ì„ ìœ„í•œ ì¹¨ë¬µ ê°ì§€ ì‹œê°„
        speech_config.set_property(speechsdk.PropertyId.SpeechServiceConnection_EndSilenceTimeoutMs, "600")       # (ë¬¸ì¥)ë¬¸ì¥ êµ¬ë¶„ì„ ìœ„í•œ ì¹¨ë¬µ ê°ì§€ ì‹œê°„
        
        speech_config.set_property_by_name("SpeechServiceConnection_RecoMode", "INTERACTIVE")  # ì‹¤ì‹œê°„ìš© ëª¨ë“œ
        #speech_config.set_property_by_name("SpeechServiceConnection_RecoMode", "CONVERSATION")  # ëŒ€í™”ìš©
        speech_config.output_format = speechsdk.OutputFormat.Simple  # ê°„ë‹¨í•œ ì¶œë ¥ í˜•ì‹

        #ì˜¤ë””ì˜¤ ì„¤ì •
        audio_format = speechsdk.audio.AudioStreamFormat(
            samples_per_second=16000, 
            bits_per_sample=16, 
            channels=1
        )
        self.audio_stream = speechsdk.audio.PushAudioInputStream(audio_format)
        audio_config = speechsdk.audio.AudioConfig(stream=self.audio_stream)

        # ìŒì„± ì¸ì‹ê¸° ìƒì„±
        self.speech_recognizer = speechsdk.SpeechRecognizer(
            speech_config=speech_config,
            audio_config=audio_config
        )

        # ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬ ì„¤ì •
        # mode - recognized : sttê°€ í•œ ë¬¸ì¥ ì¸ì‹ì„ ì™„ë£Œí–ˆì„ ë•Œì˜ ê²°ê³¼ê°’ì„ ë°˜í™˜. ì‹¤ì‹œê°„ì„±ì€ ë–¨ì–´ì§€ì§€ë§Œ í’ˆì§ˆì´ ìš°ìˆ˜.
        def recognized_handler(evt):            
            if evt.result.reason == speechsdk.ResultReason.RecognizedSpeech:    #stt ê²°ê³¼ ë°›ì•„ì˜¤ê¸° -> ì²˜ë¦¬ ìì²´ë¥¼ ë™ê¸° ë°©ì‹ìœ¼ë¡œ ì§„í–‰ (ë¹„ë™ê¸° í•¨ìˆ˜ ì‚¬ìš©x)
                text = evt.result.text.strip()
                if text:
                    print(f"ğŸ—£ï¸ ì›ë³¸: {text}\n")
                    try:
                        self.result_queue.put_nowait(text)                      #ë™ê¸° ë°©ì‹ìœ¼ë¡œ ë°˜í™˜ëœ stt ê²°ê³¼ë¥¼ queueì— ìˆœì„œëŒ€ë¡œ ì €ì¥
                    except Exception as e:
                        print(f"í ì¶”ê°€ ì˜¤ë¥˜: {e}")
                else:
                    print("ğŸ”‡ ë¹ˆ í…ìŠ¤íŠ¸ ê²°ê³¼")
            elif evt.result.reason == speechsdk.ResultReason.NoMatch:
                print("ğŸ”‡ ìŒì„± ì¸ì‹ ê²°ê³¼ ì—†ìŒ")
            else:
                print(f"ğŸ” ê¸°íƒ€ STT ê²°ê³¼: {evt.result.reason}")

        # ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬ ì„¤ì • - recognizing
        # mode - recognizing : sttê°€ ì¸ì‹í•œ ë‹¨ìœ„ì˜ ì—°ì†í•´ì„œ ë°˜í™˜. ì‹¤ì‹œì„±ì´ ìš°ìˆ˜í•˜ë‚˜ ë¹ ë¥¸ ì—…ë°ì´íŠ¸ë¡œ ë³´ê¸° ì–´ì§€ëŸ¬ìš¸ ìˆ˜ ìˆìŒ.
        def recognizing_handler(evt):            
            if evt.result.reason == speechsdk.ResultReason.RecognizingSpeech:    #stt ê²°ê³¼ ë°›ì•„ì˜¤ê¸° -> ì²˜ë¦¬ ìì²´ë¥¼ ë™ê¸° ë°©ì‹ìœ¼ë¡œ ì§„í–‰ (ë¹„ë™ê¸° í•¨ìˆ˜ ì‚¬ìš©x)
                text = evt.result.text.strip()
                if text:
                    print(f"ğŸ—£ï¸ ì›ë³¸: {text}\n")
                    try:
                        self.result_queue.put_nowait(text)                      #ë™ê¸° ë°©ì‹ìœ¼ë¡œ ë°˜í™˜ëœ stt ê²°ê³¼ë¥¼ queueì— ìˆœì„œëŒ€ë¡œ ì €ì¥
                    except Exception as e:
                        print(f"í ì¶”ê°€ ì˜¤ë¥˜: {e}")
                else:
                    print("ğŸ”‡ ë¹ˆ í…ìŠ¤íŠ¸ ê²°ê³¼")
            elif evt.result.reason == speechsdk.ResultReason.NoMatch:
                print("ğŸ”‡ ìŒì„± ì¸ì‹ ê²°ê³¼ ì—†ìŒ")
            else:
                print(f"ğŸ” ê¸°íƒ€ STT ê²°ê³¼: {evt.result.reason}")

         # ê³µí†µ í•¸ë“¤ëŸ¬ í•¨ìˆ˜ ì •ì˜ - reconizing + recognized
        
        def hybrid_result_handler(evt, is_final: bool):
            reason = evt.result.reason
            
            if reason == speechsdk.ResultReason.RecognizingSpeech or reason == speechsdk.ResultReason.RecognizedSpeech:
                text = evt.result.text.strip()

                if text:
                    result_data = {
                        'text': text,
                        'is_final': is_final
                    }
                    print(f"{'[ìµœì¢…]' if is_final else '[ì¤‘ê°„]'} {text}")
                    try:
                        self.result_queue.put_nowait(result_data)   #ë™ê¸° ë°©ì‹ìœ¼ë¡œ ë°˜í™˜ëœ stt ê²°ê³¼ë¥¼ queueì— ìˆœì„œëŒ€ë¡œ ì €ì¥
                    except Exception as e:
                        print(f"í ì¶”ê°€ ì˜¤ë¥˜: {e}")
            
            elif reason == speechsdk.ResultReason.NoMatch:
                print("ìŒì„± ì¸ì‹ ê²°ê³¼ ì—†ìŒ (NoMatch)")

        def session_started_handler(evt):
            print("ìŒì„± ì¸ì‹ ì„¸ì…˜ì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.")
            
        def session_stopped_handler(evt):
            print("ìŒì„± ì¸ì‹ ì„¸ì…˜ì´ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
            self.is_listening = False
            
        def canceled_handler(evt):
            print(f"ìŒì„± ì¸ì‹ì´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤: {evt.result.cancellation_details.reason}")
            if evt.result.cancellation_details.reason == speechsdk.CancellationReason.Error:
                print(f"ì˜¤ë¥˜ ì„¸ë¶€ì‚¬í•­: {evt.result.cancellation_details.error_details}")
            self.is_listening = False
        
        # ì´ë²¤íŠ¸ ì—°ê²°
        # stt ê²°ê³¼ê°€ ë‚˜ì™”ì„ ë•Œ,
        self.speech_recognizer.recognized.connect(lambda evt: hybrid_result_handler(evt, is_final=True))                  
        self.speech_recognizer.recognizing.connect(lambda evt: hybrid_result_handler(evt, is_final=False))

        self.speech_recognizer.session_started.connect(session_started_handler)         # ì„¸ì…˜ì´ ì‹œì‘ë˜ì—ˆì„ ë•Œ, 
        self.speech_recognizer.session_stopped.connect(session_stopped_handler)         # ì„¸ì…˜ì´ ì¢…ë£Œë˜ì—ˆì„ ë•Œ,
        self.speech_recognizer.canceled.connect(canceled_handler)                       # ì¸ì‹ì´ ì·¨ì†Œë˜ì—ˆì„ ë–„,
    
    # [3] ìŒì„± ì¸ì‹ 
    def start_recognition(self):
        """ì—°ì† ìŒì„± ì¸ì‹ ì‹œì‘"""
        
        # ì¤‘ë³µ ì¸ì‹ ë°©ì§€
        if self.is_listening:
            print("ì´ë¯¸ ìŒì„± ì¸ì‹ì´ ì§„í–‰ ì¤‘ì…ë‹ˆë‹¤.")
            return
        
        # ì—°ì† ì¸ì‹ ì‹œì‘
        self.speech_recognizer.start_continuous_recognition()   ## Azure STT audio_stream ëª¨ë‹ˆí„°ë§ ì‹œì‘ -> audio_data ì¶”ê°€ë˜ëŠ” ê²ƒ ì¸ì‹
        self.is_listening = True

    # [3-1] ì‹¤ì‹œê°„ ìŒì„± ë°›ì•„ì˜¤ê¸° : audio_streamì— ë°ì´í„° ì¶”ê°€ â†’ Azureê°€ ìë™ ê°ì§€ â†’ STT ì²˜ë¦¬ â†’ ì½œë°± í˜¸ì¶œ
    def write_audio_chunk(self, audio_data: bytes):
        """
        ì˜¤ë””ì˜¤ ì²­í¬ë¥¼ ìŠ¤íŠ¸ë¦¼ì— ì¶”ê°€
        
        Args:
            audio_data: ì˜¤ë””ì˜¤ ë°”ì´íŠ¸ ë°ì´í„°
        """
        if self.audio_stream and self.is_listening:
            self.audio_stream.write(audio_data)
            print(f"Azure STTì— ì˜¤ë””ì˜¤ ì „ì†¡: {len(audio_data)} bytes")
        else:
            print(f"Azure STT ë¹„í™œì„± ìƒíƒœ: stream={self.audio_stream is not None}, listening={self.is_listening}")
    
    # [3-2] ìŒì„± ì²˜ë¦¬ ê²°ê³¼ queue ë¹„ë™ê¸°ë¡œ ë°˜í™˜
    async def get_recognition_result(self):
        """STT ê²°ê³¼ë¥¼ ë¹„ë™ê¸°ë¡œ ë°˜í™˜"""
        try:
            return await self.result_queue.get()
        except:
            return None

    # [4] ì‹¤í–‰ ì¤‘ì§€
    def stop_recognition(self):
        """ì—°ì† ìŒì„± ì¸ì‹ ì¤‘ì§€"""
        if self.speech_recognizer and self.is_listening:
            print("\nìŒì„± ì¸ì‹ì„ ì¤‘ì§€í•©ë‹ˆë‹¤...")
            self.speech_recognizer.stop_continuous_recognition()
            self.is_listening = False
        else:
            print("ì§„í–‰ ì¤‘ì¸ ìŒì„± ì¸ì‹ì´ ì—†ìŠµë‹ˆë‹¤.")

        if self.audio_stream:
            self.audio_stream.close()
            self.audio_stream = None

        print("ìŒì„± ì¸ì‹ ì¤‘ì§€ ì™„ë£Œ")

    # [5] ìŒì„± ì¸ì‹ ì–¸ì–´ ë³€ê²½
    def change_setup_recognition(self, input_language) : 
        self.stop_recognition()                                     # ê¸°ì¡´ ì¸ì‹ ì¤‘ì§€
        self.setup_streaming_recognition(input_language)  # ìŒì„± ì¸ì‹ ì–¸ì–´ ë³€ê²½
        self.start_recognition()                                    # ë‹¤ì‹œ ì‹œì‘

    def is_active(self) -> bool:
        """í˜„ì¬ ì¸ì‹ì´ í™œì„±í™”ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸"""
        return self.is_listening